# FluxRank:一种适用于广泛部署的自动RCA框架
[点击下载原始论文](./FluxRank-paper.pdf)

索引关键词: KPI, 故障诊断, 故障定位

**本文侧重的是定位错误发生的地方，以便可以执行缓解故障的操作，不是确切的根因分析。**

FluxRank使用来自顶级搜索公司的五个真实服务（具有成千上万台计算机）的历史案例进行评估，结果表明，在70例案例中，有55（66）例的根本原因机器排名第一（前三）。 与现有方法相比，FluxRank平均将本地化时间减少了80％以上。 FluxRank已在一个Internet服务和六个Banking服务上在线部署了三个月，并且正确地将根本原因机器本地化为59例中55例中的前1例。

## 1. Introduction
实践中处理系统异常分为三步：  
1. failure confirmation: 故障确认   
2. mitigation: 故障处理  
3. Root Cause Analysis: 根因分析  
![故障处理流程](./res/pastimg-2021-03-26-11-49-51.png)

在执行错误缓解系统问题之前是很难也没有必要确定系统的根因的，这是由于以下两个原因：1. 在执行缓解措施的时候，运维人员是没有确定service问题根因的必要信息的；2. 对于现在的K8S或者Mesos构架来说，单单通过机器KPI的变化所指示的机器故障足以决策如何采取缓解措施。
### A. 现有方法的缺陷
大多数的RCA都是通过构架图来进行的。一些研究通过网络拓扑图来定位计算机网络的根因。然而软件服务的依赖图并不能像网络拓扑图那样推理出来。 Sherlock(P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz, and M. Zhang, “Towards highly reliable enterprise network services via inference of multi-level dependencies,” in ACM SIGCOMM Computer Communication Review, vol. 37, no. 4. ACM, 2007, pp. 13–24.)在每个host部署agent来推理依赖图，通过相同clients和夸不同的clients的logs的关联关系分析来定位根因。

- **MonitorRank**使用trace logs来构建依赖图，然后使用random walk算法来定位根因。  
- **CauseInfer**使用wire-captured service calls来构架服务依赖图，应用PC-algorithm来构建一个度量因果图(metric causality graph)，然后在两个级别的图上定位根因。  
- **FChain**是同Sherlock方法构架依赖图      
- **BRCA**基于服务的历史KPI异常告警来挖掘依赖图        

由于难以收集上面提到的其他必要数据，因此这些方法所需的依赖图通常在实践中很难获得。 此外，为快速变化的软件服务（尤其是在敏捷和DevOps理念下开发的软件服务）维护图形也非常具有挑战性。 代码的快速更改使依赖关系图变得难以捉摸。 结果，上述方法的应用面临很多局限性，而实际的根本原因定位仍然主要是人工，费时费力。

### B. 核心思想
使用机器学成的方法解决现实世界中的故障问题是有希望的，特别是现在的系统有许多的监控数据。然而在端到端的方法中直接使用监控数据训练机器学习模型通常效果并不好，这是因为：  
1. 没有很好的定义问题   
2. 不完整的信息     
3. 错误case不足     
4. 缺乏可解释性     

FluxRank的核心高级思想是利用领域知识（通常不会在数据中捕获或学习，但可以通过手动故障排除过程进行抽象或模仿）来设计系统, 其中每个组件都有一个定义明确的问题，足够的数据和可解释的算法的体系结构。 这样，在故障排除中看似无法解决的端到端机器学习问题就可以通过领域知识启发的体系结构解决。   

因此，FluxRank的idea是受到百度的5步故障处理大法的启发的：   
1. 在后台，运维人员使用一些在线统计算法来检测大量的服务和设备KPI的异常      
2. 当一个service失败后，运维人员可以手动搜索一下设备KPI，看看服务失败的时刻附近哪些KPI变异常了和出故障的设备   
3. 然后运维人员根据运维经验手动对故障设备潜在的组合进行排名     
4. 运维人员手动在排名较高的设备组合上依次触发自动化动作（将流量切离故障设备或重启故障设备）     
5. 当故障处理成功后（after a successful mitigation）， 开发工程师（非运维人员）将花时间分析代码和logs，以找到和修复最终的根因(root cause)       

在上述方法中，只需要定位根因设备并将其从服务中异常就可以缓解故障，对于采用分布式集群的高可用架构来说，这个优势非常明显。

然而上述方法中，第一步是低效的、不准确的，第二步和第三步是手动的，因此故障恢复花费的时间太长。**作者分析了一个服务（运行在11519台设备上的29个模块）9个月的8起使用上述方法进行故障修复的记录，最大的修复时间是175分钟，平均59分钟。**

受到上述实践的启发并意识到上述方法的低效性，本文提出了一个通用系统**FluxRank**，可以更具给定的软件无法失败信息自动定位根因设备。FluxRank使用同样的设备KPIs指标数据(几乎所有的软件服务中都是使用这些数据的），并且效仿上述方法的前三个步骤，但是让每个步骤高效(efficient)、准确(accurate)和自动(automatic)，从而大大减少故障恢复的时间。

![Overview of FluxRank](./res/pastimg-2021-03-31-10-11-46.png)

FluxRank是通过服务失败触发的（KPI异常检测不属于本文的探讨范围）。**FluxRank的输入包括：服务故障时刻和所有的设备KPI数据。FluxRank有三个阶段：change quantification，digest distillation，digest ranking**。**首先,**在*change quantification*阶段所有的设备KPIs指标被量化；**第二,**在*digest distillation*阶段，所有的KPI通过聚类算法分类为*digest*，每个*digest*都包含了来自于同一模块的一组设备和代表模块异常模式的一组KPI。**第三,**所有的*digest*更具在*digest ranked*阶段的潜在根因定位自动排名。**最后,**基于digest排序的结果，运维人员通过触发一些自动化动作进行故障缓解。*change quantification*模仿了步骤2中的手动处理过程，*digest distillation*和*digest ranking*模仿了第三步。

### C. 挑战和贡献
#### 挑战1：如何快速量化服务故障时间内大量不同的KPI变化？
虽然这些年已经提出了许多异常检测算法，每个被监控的KPI需要单独进行算法选择和调参。当KPI的数量达、种类多的时候，谨慎的进行算法选择和参数优化变得不可行，导致异常检测的结果不准确，在step2和step3中遇到更多的困难。但是，不同于在任何时候都可能发生异常的一般异常检测方案，在我们的方案中已经给出了服务故障时间，我们只需要开发一种算法即可量化服务故障时间附近机器KPI的变化。 剩下的挑战是该算法必须具有灵活性以在各种KPI上工作，并且必须足够轻巧和快速，以便能够快速分析大量KPI。

#### 挑战2：如何对具有物理意义的KPI进行聚类？
一个模块中的故障可以传播到其他模块，并且故障机器可能同时具有多个异常的KPI。 结果，一个服务故障通常与许多机器KPI异常同时发生。 运维人员经常会根据他们的领域知识以临时的方式总结机器的KPI异常。 我们的挑战是以一种对运维人员来说很直观的方式来确定缓解措施，从而在相关机器/模块上自动，系统地将具有相似异常程度的KPI聚类。

#### 挑战3：如何对聚类结果进行排序？
对于大型软件服务，通常在计设备别经常存在并发但不相关的异常情况。 这意味着，对于服务故障，第2阶段以上可能会输出多个聚类结果。 因此，在阶段3中，我们必须对阶段2中的聚类结果进行排名，以便与故障最相关的聚类结果应出现在顶部。 但是，之前没有研究过排序聚类（在不同机器上具有不同异常程度的不同KPI），并且没有已知的指标或排序算法可以直接处理此类聚类结果。

本文具有以下贡献：
#### 贡献1
对于挑战1的，本文提出了一种非参数的轻量级的基于KDE(Kernel Density Estimation)的算法用于在指定时刻周围的大量的、多样的KPI指标数据变化的量化和对比。

#### 贡献2
对于挑战2，提出了一种变化的向量表示、距离方程和基于DBSCAN的聚类算法把KPIs聚类为不同的*digest*来表示模块的异常模式。

#### 贡献3
针对挑战3，提出了特征向量和基于排序算法的逻辑回归来进行RCD(Root Cause Digest)排序

#### 贡献4
在5个生产服务（数万台的设备）的70个真实的离线故障上实验，该RCDs方法在55例中排名第一，在64例中排名第三。与现有方法相比，FluxRank平均将定位时间时间减少了80％以上。

#### 贡献5
FluxRank是一个可以广泛部署的框架。

## 2. Change Quantification
在*Change Quantification*阶段，我们试图将通过*change degress*测量的设备KPI的变化进行量化。*Change degrees*可以用来对比不同类型的KPI. 另外，当故障发生时根因设备的KPI首先发生变化，受该故障影响的其他的KPI随之发生变化。因此变化开始的时间(*change start time* $T_c$)对于定位根因设备很有帮助。

所以很清晰的一点是，**change quantification的设计目标就是对于大量的多样的KPI迅速而准确的确定change start time和确定服务故障时的变化度(change degree)**。回想一下，传统的异常检测算法无法实现上述目标，因为它们在大量不同KPI的算法选择和参数调整中非常费力。 因此，我们建议在变更量化中使用两步设计：（1）应用绝对导数来识别变更开始时间；（2）使用核密度估计（KDE）来确定变更程度。

### A. Change Start Time
找出KPI指标的change start time($T_c$)问题可以转化为经典的change point detection问题。

